{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## Load libraries and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/MLAKSDeployment/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/MLAKSDeployment/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/MLAKSDeployment/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/MLAKSDeployment/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/MLAKSDeployment/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/MLAKSDeployment/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "from ItemSelector import ItemSelector\n",
    "from label_rank import label_rank\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input parameters.\n",
    "One of the most important parameters is the number of estimators that allows you to trade-off accuracy, modeling time, and model size. The table below should give you an idea of the relationships between the number of estimators and the metrics.\n",
    "\n",
    "| Estimators | Run time (s) | Size (MB) | Accuracy@1 | Accuracy@2 | Accuracy@3 |\n",
    "|------------|--------------|-----------|------------|------------|------------|\n",
    "|        100 |           40 |  2 | 25.02% | 38.72% | 47.83% |\n",
    "|       1000 |          177 |  4 | 46.79% | 60.80% | 69.11% |\n",
    "|       2000 |          359 |  7 | 51.38% | 65.93% | 73.09% |\n",
    "|       4000 |          628 | 12 | 53.39% | 67.40% | 74.74% |\n",
    "|       8000 |          904 | 22 | 54,62% | 67.77% | 75.35% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train_data = 'balanced_pairs_train.tsv' # The file of training data.\n",
    "args_test_data = 'balanced_pairs_test.tsv'   # The file of testing data.\n",
    "args_estimators = 8000                       # The number of estimators fit by LightGBM.\n",
    "args_min_child_samples = 20                  # The minimum number of samples in a leaf created bty LightGBM.\n",
    "args_verbose = -1                            # The progress report messages from LightGBM; \"-1\" means none.\n",
    "args_ngrams = 1                              # The maximum size of ngrams created by TfidfVectorizer.\n",
    "args_unweighted = False                      # Whether to ignore instance weights used to correct imbalance in training.\n",
    "args_match = 20                              # The maximum number of original questions per duplicate to use in the data. \n",
    "args_outputs = '.'                           # The folder where this notebook deposits its outputs.\n",
    "args_inputs = '.'                            # The folder where this notebook picks up its inputs.\n",
    "args_save = True                             # Whether to save the model created by the notebook.\n",
    "args_model = 'model.pkl'                     # The file containing the saved model.\n",
    "args_instances = 'inst.txt'                  # The file containing the scored test data.\n",
    "args_labels = 'labels.txt'                   # The file containing the ordered unique ids of the original questions's answer ids. \n",
    "args_rank = 3                                # The maximum position at which to report test set accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths to the notebook's input and output files\n",
    "\n",
    "The training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_path = args_inputs\n",
    "data_path = os.path.join(inputs_path, args_train_data)\n",
    "test_path = os.path.join(inputs_path, args_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved model file and the scored test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_path = args_outputs\n",
    "os.makedirs(outputs_path, exist_ok=True)                     # Create the outputs folder.\n",
    "model_path = os.path.join(outputs_path, args_model)\n",
    "instances_path = os.path.join(outputs_path, args_instances)\n",
    "labels_path = os.path.join(outputs_path, args_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and set up the training data\n",
    "\n",
    "Load the training data, and display a sample of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./balanced_pairs_train.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_x</th>\n",
       "      <th>AnswerId_x</th>\n",
       "      <th>Text_x</th>\n",
       "      <th>Id_y</th>\n",
       "      <th>Text_y</th>\n",
       "      <th>AnswerId_y</th>\n",
       "      <th>Label</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>336859</td>\n",
       "      <td>var functionname = function() {} vs function f...</td>\n",
       "      <td>336868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>572897</td>\n",
       "      <td>how does javascript .prototype work?. i'm not ...</td>\n",
       "      <td>572996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>29986657</td>\n",
       "      <td>global variable usage on page reload. i am bas...</td>\n",
       "      <td>30070207</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>1085801</td>\n",
       "      <td>get selected value in dropdown list using java...</td>\n",
       "      <td>1085810</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>8228281</td>\n",
       "      <td>what is the (function() { } )() construct in j...</td>\n",
       "      <td>8228308</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id_x  AnswerId_x                                             Text_x  \\\n",
       "0  114525      336868  the difference between the two functions? (\"fu...   \n",
       "1  114525      336868  the difference between the two functions? (\"fu...   \n",
       "2  114525      336868  the difference between the two functions? (\"fu...   \n",
       "3  114525      336868  the difference between the two functions? (\"fu...   \n",
       "4  114525      336868  the difference between the two functions? (\"fu...   \n",
       "\n",
       "       Id_y                                             Text_y  AnswerId_y  \\\n",
       "0    336859  var functionname = function() {} vs function f...      336868   \n",
       "1    572897  how does javascript .prototype work?. i'm not ...      572996   \n",
       "2  29986657  global variable usage on page reload. i am bas...    30070207   \n",
       "3   1085801  get selected value in dropdown list using java...     1085810   \n",
       "4   8228281  what is the (function() { } )() construct in j...     8228308   \n",
       "\n",
       "   Label  n  \n",
       "0      1  0  \n",
       "1      0  1  \n",
       "2      0  2  \n",
       "3      0  3  \n",
       "4      0  4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Reading {}'.format(data_path))\n",
    "train = pd.read_csv(data_path, sep='\\t', encoding='latin1')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the number of duplicate-original question matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.n < args_match]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the roles of the columns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['Text_x', 'Text_y']\n",
    "label_column = 'Label'\n",
    "duplicates_id_column = 'Id_x'\n",
    "answer_id_column = 'AnswerId_y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report on the training dataset: the number of rows and the proportion of true matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 132,500 rows with 5.00% matches\n"
     ]
    }
   ],
   "source": [
    "print('train: {:,} rows with {:.2%} matches'.format(\n",
    "      train.shape[0], train[label_column].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the instance weights used to correct for class imbalance in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_column = 'Weight'\n",
    "if args_unweighted:\n",
    "    weight = pd.Series([1.0], train[label_column].unique())\n",
    "else:\n",
    "    label_counts = train[label_column].value_counts()\n",
    "    weight = train.shape[0]/(label_counts.shape[0]*label_counts)\n",
    "train[weight_column] = train[label_column].apply(lambda x: weight[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the unique ids that identify each original question's answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(train[answer_id_column].unique())\n",
    "label_order = pd.DataFrame({'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model.\n",
    "\n",
    "Collect the parts of the training data by role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[feature_columns]\n",
    "train_y = train[label_column]\n",
    "sample_weight = train[weight_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the inputs to define the hyperparameters used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = args_estimators\n",
    "min_child_samples = args_min_child_samples\n",
    "if args_ngrams > 0:\n",
    "    ngram_range = (1, args_ngrams)\n",
    "else:\n",
    "    ngram_range = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the hyperparameter values are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_estimators > 0\n",
    "assert min_child_samples > 1\n",
    "assert type(ngram_range) is tuple and len(ngram_range) == 2\n",
    "assert ngram_range[0] > 0 and ngram_range[0] <= ngram_range[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the pipeline that featurizes the text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurization = [\n",
    "    (column,\n",
    "     make_pipeline(ItemSelector(column),\n",
    "                   text.TfidfVectorizer(ngram_range=ngram_range)))\n",
    "    for column in feature_columns]\n",
    "features = FeatureUnion(featurization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the estimator that learns how to classify duplicate-original question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = lgb.LGBMClassifier(n_estimators=n_estimators,\n",
    "                               min_child_samples=min_child_samples,\n",
    "                               verbose=args_verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model pipeline as feeding the features into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('features', features),\n",
    "    ('model', estimator)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model.\n",
    "This step should take about seven and a half minutes on a Standard NC6 DLVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(train_X, train_y, model__sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a file, and report on its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model.pkl size: 22.41 MB\n"
     ]
    }
   ],
   "source": [
    "if args_save:\n",
    "    joblib.dump(model, model_path)\n",
    "    print('{} size: {:.2f} MB'.format(model_path, os.path.getsize(model_path)/(2**20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Read in the test data set, and report of the number of its rows and proportion of true matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./balanced_pairs_test.tsv\n",
      "test 297,570 rows with 0.55% matches\n"
     ]
    }
   ],
   "source": [
    "print('Reading {}'.format(test_path))\n",
    "test = pd.read_csv(test_path, sep='\\t', encoding='latin1')\n",
    "print('test {:,} rows with {:.2%} matches'.format(\n",
    "    test.shape[0], test[label_column].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a sample of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_x</th>\n",
       "      <th>AnswerId_x</th>\n",
       "      <th>Text_x</th>\n",
       "      <th>Id_y</th>\n",
       "      <th>Text_y</th>\n",
       "      <th>AnswerId_y</th>\n",
       "      <th>Label</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>336859</td>\n",
       "      <td>var functionname = function() {} vs function f...</td>\n",
       "      <td>336868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>1885557</td>\n",
       "      <td>simplest code for array intersection in javasc...</td>\n",
       "      <td>1885660</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>2100758</td>\n",
       "      <td>javascript or (||) variable assignment explana...</td>\n",
       "      <td>2100767</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>3384504</td>\n",
       "      <td>location of parenthesis for auto-executing ano...</td>\n",
       "      <td>3384534</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>15141762</td>\n",
       "      <td>how to initialize javascript date to a particu...</td>\n",
       "      <td>15171030</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id_x  AnswerId_x                                             Text_x  \\\n",
       "0  114525      336868  the difference between the two functions? (\"fu...   \n",
       "1  114525      336868  the difference between the two functions? (\"fu...   \n",
       "2  114525      336868  the difference between the two functions? (\"fu...   \n",
       "3  114525      336868  the difference between the two functions? (\"fu...   \n",
       "4  114525      336868  the difference between the two functions? (\"fu...   \n",
       "\n",
       "       Id_y                                             Text_y  AnswerId_y  \\\n",
       "0    336859  var functionname = function() {} vs function f...      336868   \n",
       "1   1885557  simplest code for array intersection in javasc...     1885660   \n",
       "2   2100758  javascript or (||) variable assignment explana...     2100767   \n",
       "3   3384504  location of parenthesis for auto-executing ano...     3384534   \n",
       "4  15141762  how to initialize javascript date to a particu...    15171030   \n",
       "\n",
       "   Label  n  \n",
       "0      1  0  \n",
       "1      0  1  \n",
       "2      0  2  \n",
       "3      0  3  \n",
       "4      0  4  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the model predictions. This step should take about 1 minute on a Standard NC6 DLVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 17s, sys: 1.02 s, total: 4min 18s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_X = test[feature_columns]\n",
    "test['probabilities'] = model.predict_proba(test_X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the sample with the added probabilities column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_x</th>\n",
       "      <th>AnswerId_x</th>\n",
       "      <th>Text_x</th>\n",
       "      <th>Id_y</th>\n",
       "      <th>Text_y</th>\n",
       "      <th>AnswerId_y</th>\n",
       "      <th>Label</th>\n",
       "      <th>n</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>336859</td>\n",
       "      <td>var functionname = function() {} vs function f...</td>\n",
       "      <td>336868</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.999878e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>1885557</td>\n",
       "      <td>simplest code for array intersection in javasc...</td>\n",
       "      <td>1885660</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.680693e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>2100758</td>\n",
       "      <td>javascript or (||) variable assignment explana...</td>\n",
       "      <td>2100767</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.683527e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>3384504</td>\n",
       "      <td>location of parenthesis for auto-executing ano...</td>\n",
       "      <td>3384534</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7.026458e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>15141762</td>\n",
       "      <td>how to initialize javascript date to a particu...</td>\n",
       "      <td>15171030</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.369285e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id_x  AnswerId_x                                             Text_x  \\\n",
       "0  114525      336868  the difference between the two functions? (\"fu...   \n",
       "1  114525      336868  the difference between the two functions? (\"fu...   \n",
       "2  114525      336868  the difference between the two functions? (\"fu...   \n",
       "3  114525      336868  the difference between the two functions? (\"fu...   \n",
       "4  114525      336868  the difference between the two functions? (\"fu...   \n",
       "\n",
       "       Id_y                                             Text_y  AnswerId_y  \\\n",
       "0    336859  var functionname = function() {} vs function f...      336868   \n",
       "1   1885557  simplest code for array intersection in javasc...     1885660   \n",
       "2   2100758  javascript or (||) variable assignment explana...     2100767   \n",
       "3   3384504  location of parenthesis for auto-executing ano...     3384534   \n",
       "4  15141762  how to initialize javascript date to a particu...    15171030   \n",
       "\n",
       "   Label  n  probabilities  \n",
       "0      1  0   9.999878e-01  \n",
       "1      0  1   2.680693e-07  \n",
       "2      0  2   6.683527e-08  \n",
       "3      0  3   7.026458e-02  \n",
       "4      0  4   2.369285e-10  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the probabilities for each duplicate question, ordered by the original question ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the testing data by duplicate question id and original question id.\n",
    "test.sort_values([duplicates_id_column, answer_id_column], inplace=True)\n",
    "\n",
    "# Extract the ordered probabilities.\n",
    "probabilities = (\n",
    "    test.probabilities\n",
    "    .groupby(test[duplicates_id_column], sort=False)\n",
    "    .apply(lambda x: tuple(x.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with one row per duplicate question, and make it contain the model's predictions for each duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = (test[['Id_x', 'AnswerId_x', 'Text_x']]\n",
    "              .drop_duplicates()\n",
    "              .set_index(duplicates_id_column))\n",
    "test_score['probabilities'] = probabilities\n",
    "test_score.reset_index(inplace=True)\n",
    "test_score.columns = ['Id', 'AnswerId', 'Text', 'probabilities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a sample of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AnswerId</th>\n",
       "      <th>Text</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114525</td>\n",
       "      <td>336868</td>\n",
       "      <td>the difference between the two functions? (\"fu...</td>\n",
       "      <td>(1.2918279344766618e-08, 3.14020517731647e-07,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>559752</td>\n",
       "      <td>242833</td>\n",
       "      <td>single quotes versus double quotes in js.  pos...</td>\n",
       "      <td>(4.600735020435786e-07, 9.751980173988337e-06,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>832257</td>\n",
       "      <td>17606289</td>\n",
       "      <td>javascript multiple replace. how do you replac...</td>\n",
       "      <td>(2.424455524526655e-06, 2.369740101672909e-06,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>894696</td>\n",
       "      <td>1520853</td>\n",
       "      <td>why is the javascript regexp.test() method beh...</td>\n",
       "      <td>(2.5903996990707457e-08, 3.940605834411278e-09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242481</td>\n",
       "      <td>6055620</td>\n",
       "      <td>copy to clipboard using javascript.  possible ...</td>\n",
       "      <td>(5.685098071054511e-09, 3.0032531359901773e-09...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  AnswerId                                               Text  \\\n",
       "0   114525    336868  the difference between the two functions? (\"fu...   \n",
       "1   559752    242833  single quotes versus double quotes in js.  pos...   \n",
       "2   832257  17606289  javascript multiple replace. how do you replac...   \n",
       "3   894696   1520853  why is the javascript regexp.test() method beh...   \n",
       "4  1242481   6055620  copy to clipboard using javascript.  possible ...   \n",
       "\n",
       "                                       probabilities  \n",
       "0  (1.2918279344766618e-08, 3.14020517731647e-07,...  \n",
       "1  (4.600735020435786e-07, 9.751980173988337e-06,...  \n",
       "2  (2.424455524526655e-06, 2.369740101672909e-06,...  \n",
       "3  (2.5903996990707457e-08, 3.940605834411278e-09...  \n",
       "4  (5.685098071054511e-09, 3.0032531359901773e-09...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the predictions\n",
    "\n",
    "For each duplicate question, find the rank of its correct original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score['Ranks'] = test_score.apply(lambda x:\n",
    "                                       label_rank(x.AnswerId,\n",
    "                                                  x.probabilities,\n",
    "                                                  label_order.label),\n",
    "                                       axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the fraction of correct original questions by minimum rank. Also print the average rank of the correct original questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @1 = 54.31%\n",
      "Accuracy @2 = 68.81%\n",
      "Accuracy @3 = 75.78%\n",
      "Mean Rank 5.2807\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, args_rank+1):\n",
    "    print('Accuracy @{} = {:.2%}'.format(\n",
    "        i, (test_score['Ranks'] <= i).mean()))\n",
    "mean_rank = test_score['Ranks'].mean()\n",
    "print('Mean Rank {:.4f}'.format(mean_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the scored instances to a file, along with the ordered original questions's answer ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score.to_csv(instances_path, sep='\\t', index=False,\n",
    "                  encoding='latin1')\n",
    "label_order.to_csv(labels_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will [develop the model API](02_Develop_Model_Driver.ipynb) to call our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployment]",
   "language": "python",
   "name": "conda-env-MLAKSDeployment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
